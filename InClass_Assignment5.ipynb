{
 "cells": [
  {
   "cell_type": "raw",
   "id": "272fc420-0d80-4424-81fd-2733be667345",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"In-Class Assignment 5\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    embed-resources: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa0c152-1c41-4dee-b9f0-1ea117a0d20a",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "\n",
    "- For questions that require coding, you need to write the relevant code and display its output. Your output should either be the direct answer to the question or clearly display the answer in it.\n",
    "- For questions that require a written answer (sometimes along with the code), you need to put your answer in a Markdown cell. Writing the answer as a comment or as a print line is not acceptable.\n",
    "- You need to render this file as HTML using Quarto and submit the HTML file. **Please note that this is a requirement and not optional.** A submission cannot be graded until it is properly rendered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4076ed8-bbc4-4827-9542-836ac6663fff",
   "metadata": {},
   "source": [
    "Import all the libraries and tools you need below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940fec5f-6d14-4be9-88f4-9adda83b8530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Run the line below to install the xgboost library. It is not in Anaconda by default.\n",
    "!pip install xgboost\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5401ef3-3934-43c8-b0d1-230dc8ad5072",
   "metadata": {},
   "source": [
    "In this assignment, you will use the data from the **cirrhosis_outcomes.csv** file. Each observation is a patient with liver cirrhosis. \n",
    "\n",
    "- The `Status` variable represents the survival state of the patient at `N-Days`: `C` for censored (alive), `D` for death and `CL` for censored (alive) with liver transplant.\n",
    "- All other variables are medical predictors, either about the treatment or the patient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486ae9dd-45c3-4d58-80e3-2d872c6f3947",
   "metadata": {},
   "source": [
    "## 1) Preprocessing (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe76b73-9536-4362-bbd2-34937e0efe1f",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "Read the data. Use `index_col=0` to assign the `id` variable to the index; it should not be a predictor. **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f75da6-70fe-444d-bfe0-722332559ad8",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "`Status` will be the response (target) variable for the classification task. Print the `value_counts` of the classes. Are the classes balanced? Which one is the minority class? **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4933ccf-81bf-4c40-b75b-b0b4ab1fdd1a",
   "metadata": {},
   "source": [
    "### c)\n",
    "\n",
    "`map` the class labels to 0, 1 and 2. This is necessary because some models that are included do not recognize non-numeric input. **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc801135-c307-46a2-b399-8255990e3fc5",
   "metadata": {},
   "source": [
    "### d)\n",
    "\n",
    "- Separate the response and the predictors. All variables other than `Status` should be a predictor.\n",
    "- One-hot-encode the categorical predictors. (This can and should be done with one function in one line.) Use `drop_first=True`.\n",
    "- Create the training and test data with an 80%-20% split. **Stratify the data.** Use `random_state=42`. \n",
    "- Scale the training and the test data.\n",
    "\n",
    "**(6 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb964ea4-0690-4516-a0d2-2ade33039f28",
   "metadata": {},
   "source": [
    "## 2) Tuning and Evaluating Different Multi-Class Classifiers (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8880ed79-1bb9-4b2c-af55-f84d0962b733",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "Create four models with the specified inputs:\n",
    "\n",
    "- A [Logistic Regression](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html) model: Use `multi_class = 'ovr'`, `solver = liblinear` and `random_state=1`.\n",
    "- A [Linear SVC](https://scikit-learn.org/dev/modules/generated/sklearn.svm.LinearSVC.html): Use the `LinearSVC` object for efficiency reasons. Use `multi_class = 'ovr'` and `random_state=1`.\n",
    "- A [KNN (K-Nearest Neighbors)](https://scikit-learn.org/1.5/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) classifier: Do not use any inputs.\n",
    "- An [XGBoost](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn) classifier: Use `random_state=1`. Do not use any other inputs.\n",
    "\n",
    "**(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a92c3e-c088-423a-8eb4-5d1f77d6dd4d",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "Note that the links to the model documentations are given in Part a. Using the documentations, answer the following questions:\n",
    "\n",
    "- Do you see a `multi-class` input option for the models that did not take any such input in Part a? Why is that the case? (Only consider the scikit-learn API for XGBoost and disregard the experimental/work-in-progress inputs; they are not fully developed yet.)\n",
    "- Among the models that took a `multi_class` input, `ovr` is an option along with some other algorithms. Is **OvO** (One vs One) one of the options? Why do you think this is the case?\n",
    "\n",
    "**(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366b7fc9-30a0-443b-a9bb-2b9cc047e5da",
   "metadata": {},
   "source": [
    "### c)\n",
    "\n",
    "Using the given hyperparameter grids and the following specifications, tune and evaluate each model:\n",
    "\n",
    "- Use `cv=5`. The default classification setting of `GridSearchCV` is stratification. (The object requirement in the previous in-class assignment was to get everyone familiar with the usage of those cross-validation setting objects.)\n",
    "- Use `f1_macro` for scoring. F1-score is calculated as: $$2*\\frac{precision*recall}{precision+recall}$$ The macro f1-score uses the macro precision and recall scores. It is a good metric to use if you want to tune your model with both precision and recall.\n",
    "- Print the cross-validation performance of the best model (`best_score_`).\n",
    "- Print the `confusion_matrix` and the `classification_report` for the test data.\n",
    "- Print the **micro** recall score for the test data.\n",
    "\n",
    "**(20 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3071f594-42e9-41c3-b021-39443d52633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lr = {\n",
    "    'penalty': [None, 'l1', 'l2', 'elasticnet'],\n",
    "    'l1_ratio': [0, 0.3, 0.6, 1],\n",
    "    'C': [0.01,0.1,1,10,100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b7bff9-c10b-4f84-9cb4-e4c8b85000c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_svm = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d4dc16-a839-4ac1-8215-05d818a72a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': np.arange(1,25,2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f271f64b-734e-4cb1-9878-e22ceb57defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.1, 0.01, 0.001]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd8e01-950c-4720-8006-d5d0188d5616",
   "metadata": {},
   "source": [
    "## 3) Interpretation (45 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a4c2b1-0eba-42a7-9539-e21aa38622a4",
   "metadata": {},
   "source": [
    "Using the prediction results of all four models, answer the following questions. **You need to justify your answers with the corresponding results for credit.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de2b6a-edb3-4d68-9ed7-66693a5ac41f",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "In this classification task, what is the random baseline accuracy that the accuracy values would be compared against? **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c733fac7-263b-4616-88e9-18ec62c5723d",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "How do the linear models handle the minority class? What do the False Negatives (FNs) and False Positives (FPs) of the minority class indicate about the linear models' capacity to handle the minority class? **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45399358-42d1-46a7-8f29-7bb7e4102367",
   "metadata": {},
   "source": [
    "### c)\n",
    "\n",
    "Is there a considerable difference between the micro and macro recall scores for all models? Why or why not? **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa665e7-0261-46c0-b8ac-fc6738e3f105",
   "metadata": {},
   "source": [
    "### d)\n",
    "\n",
    "Compare the test accuracies of the linear models with the KNN classifier. Which one has a higher accuracy? Is accuracy a useful metric to evaluate the model performance in this case, especially regarding the minority class? Why or why not? **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a7d02-08f8-4e40-aa83-0ceea11e6acc",
   "metadata": {},
   "source": [
    "### e)\n",
    "\n",
    "Which model performs the best overall? How does its performance still change with the support (number of observations) of each class? What do you think can be done to overcome this persistent issue? (You will explore some options in this regard in Homework Assignment 2.) **(10 points)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
